---
layout: post
title: (ì¶”ì²œì‹œìŠ¤í…œ) í˜‘ì—… í•„í„°ë§ -ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì²œ
tags: [ì¶”ì²œì‹œìŠ¤í…œ]
math: true
date: 2021-01-13 22:55 
comments : true
---

í•´ë‹¹ ê¸€ì€ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ê³µê°œí•œ í˜‘ì—… í•„í„°ë§ jupyter notebookì˜ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤. 


ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ê³µê°œí•œ ì½”ë“œë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ --> [ì—¬ê¸°](https://github.com/microsoft/recommenders)

ì˜¤ëŠ˜ ì •ë¦¬í•  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
- í˜‘ì—… í•„í„°ë§ì˜ ê°œë… 
- í˜‘ì—… í•„í„°ë§ (ì½”ë“œ) : microsoftì˜ reco_util ëª¨ë“ˆ í™œìš© 

---
## í˜‘ì—… í•„í„°ë§(Collaborative Filtering) ì˜ ê°œìš” 

âœ” í˜‘ì—… í•„í„°ë§ì´ë€, ì‚¬ìš©ìì™€ ì œí’ˆ ê°„ì˜ `ìƒí˜¸ì‘ìš©` ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìœ ì €ì—ê²Œ ì´ì „ì— ì¢‹ì•„í–ˆë˜ ì œí’ˆê³¼ ìœ ì‚¬í•œ ì œí’ˆì„ ì¶”ì²œí•˜ê±°ë‚˜(item-Based Collaborative Filtering)ê³¼ ìœ ì €ì˜ ì·¨í–¥ê³¼ ìœ ì‚¬í•œ ì·¨í–¥ì„ ê°€ì§„ ê³ ê°ì´ ì¢‹ì•„í•˜ëŠ” ì œí’ˆì„ ì¶”ì²œí•˜ëŠ” ë°©ì‹(User-Based Collaborative Filtering)ì„ ë§í•©ë‹ˆë‹¤. 

- User-Based CF : ëˆ„ê°€, ë¬´ì—‡ì„ ì–¼ë§ˆë‚˜ ì¢‹ì•„í•˜ëŠ”ì§€ í‘œí˜„ (ì‚¬ìš©ìID,ì•„ì´í…œID,ì„ í˜¸ê°’)
- Item-Based CF : ì•„ì´í…œê³¼ ì•„ì´í…œì´ ì–¼ë§ˆë‚˜ ì—°ê´€ì´ ìˆëŠ”ì§€ í‘œí˜„ (ì•„ì´í…œID,ì•„ì´í…œID,ì„ í˜¸ê°’)

ì‚¬ìš©ìëŠ” ê³¼ê±°ì— ìƒí˜¸ì‘ìš©í•œ(êµ¬ë§¤)í•œ ì  ìˆëŠ” ì•„ì´í…œê³¼ ìœ ì‚¬í•œ ì•„ì´í…œì„ ***ì„ í˜¸***í•©ë‹ˆë‹¤. 

### í˜‘ì—… í•„í„°ë§ ìœ ì˜ì‚¬í•­
- ì‚¬ìš©ì/ì•„ì´í…œì˜ í”¼ì³(feature) ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜(ì˜ˆ:Neural CF ë“±)ì— ë¹„í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- ë©”ëª¨ë¦¬ ë¶€ì¡± í˜„ìƒ : ì‚¬ìš©ì/ì•„ì´í…œì˜ ìµœì†Œ ê¸°ì¤€ ì„ ì • í›„ ì ìš© í•„ìš”
- `í‰ì ê³¼ ê°™ì´ ì§ì ‘ì ì¸ ì„ í˜¸ë„ ê´€ë ¨ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ì—ëŠ”, ì„ í˜¸ê°’ì— ëŒ€í•œ ì •ì˜ í•„ìš” ` <br>
   ì˜ˆ) êµ¬ë§¤ìˆ˜, í´ë¦­ìˆ˜, ì¥ë°”êµ¬ë‹ˆ íšŸìˆ˜ ë“± 


<í˜‘ì—… í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ ê°œë…ë„(***simple***)>

<img src="https://recodatasets.blob.core.windows.net/images/sar_schema.svg?sanitize=true">
---

## Code
- microsoft ì˜ reco_util ëª¨ë“ˆ í™œìš©
- `ì•„ë˜ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ì‹¶ìœ¼ì‹œë©´, microsoft recommender git clone ì´í›„ ì‹¤í–‰í•´ì£¼ì„¸ìš”`ğŸ˜€
>  git clone https://github.com/Microsoft/Recommenders   

```python
# íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— í•­ìƒ ëª¨ë“  ëª¨ë“ˆì„ Reload
import autoreload
%load_ext autoreload
%autoreload 2

# ê²½ë¡œ ì§€ì • 
import sys
sys.path.append("../../")

os.chdir('e:/github/recommender_github/Recommenders')

import logging
import numpy as np
import pandas as pd

import scrapbook as sb # notebookì—ì„œ ì‹¤í–‰í•œ ë°ì´í„°ì˜ ê°’ì´ë‚˜ ì‹œê°í™” ì»¨í…ì¸ ë¥¼ ê¸°ë¡ 
# https://nteract-scrapbook.readthedocs.io/en/latest/

from sklearn.preprocessing import minmax_scale

from reco_utils.common.python_utils import binarize
from reco_utils.common.timer import Timer
from reco_utils.dataset import movielens
from reco_utils.dataset.python_splitters import python_stratified_split
from reco_utils.evaluation.python_evaluation import (
    map_at_k,
    ndcg_at_k,
    precision_at_k,
    recall_at_k,
    rmse,
    mae,
    logloss,
    rsquared,
    exp_var
)
from reco_utils.recommender.sar import SAR

print("System version: {}".format(sys.version))
print("Pandas version: {}".format(pd.__version__))
```

### 1 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

í˜‘ì—… í•„í„°ë§ì— í•„ìš”í•œ ë°ì´í„°
-  `<User ID>, <Item ID>,<Time>,[<Event Type>], [<Event Weight>]`

ê° í–‰ì€ ì‚¬ìš©ìì™€ ì•„ì´í…œ ê°„ì˜ í•˜ë‚˜ì˜ ìƒí˜¸ì‘ìš©ì„ ì˜ë¯¸í•œë‹¤. ì´ ìƒí˜¸ì‘ìš©ì€ e-commerce ë¶„ì•¼ì—ì„œ ìƒí’ˆì„ í´ë¦­í•˜ê±°ë‚˜, ì¥ë°”êµ¬ë‹ˆì— ì¶”ê°€í•˜ê±°ë‚˜, ì¶”ì²œ ë§í¬ë¥¼ ì—°ê²°í•˜ê±°ë‚˜, ê¸°íƒ€ ë“±ë“±ì˜ í–‰ë™ì„ ì˜ë¯¸í•œë‹¤. ê°ê°ì˜ event_typeì€ ê°ê¸° ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, êµ¬ë§¤ ì´ë²¤íŠ¸ë¥¼ 10, viewingì€ 1ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

MovieLens ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ì‹¤ìŠµí•´ë³´ì. 

```python
# ì¶”ì²œ ì•„ì´í…œì˜ ê°œìˆ˜ë¥¼ ì„ ì • 
TOP_K = 10 

# MovieLens ë°ì´í„°ì…‹ì˜ size ì„ íƒ : 100k, 1m, 10m, or 20m
MOVIELENS_DATA_SIZE = '100k'

# MovieLens Dataset ë‹¤ìš´ë¡œë“œ --> movielens.py 

data = movielens.load_pandas_df(
    size = MOVIELENS_DATA_SIZE
)

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ floatë¥¼ 32bitë¡œ ë³€ê²½
data['rating'] = data['rating'].astype(np.float32)

# data í™•ì¸í•´ë³´ê¸°
print(data.shape) # (100000, 4)
print('userIDì˜ uniqueí•œ ê°œìˆ˜ëŠ”:{}'.format(data['userID'].nunique()))
print('itemID uniqueí•œ ê°œìˆ˜ëŠ”:{}'.format(data['itemID'].nunique()))

```
### 2 ë°ì´í„° ë¶„í•  (train/test)
train/test ë¶„í•  ì‹œ, testì…‹ì— ì¡´ì¬í•˜ëŠ” user_idëŠ” training ì…‹ì—ì„œë„ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤

```python
# python_splitter.py í•¨ìˆ˜ ì´ìš©í•´ì„œ ë°ì´í„° ë¶„í• 
train,test = python_stratified_split(data,
ratio=0.75,
col_user = 'userID',
col_item = 'itemID',
seed = 42
)
```
### 3 ëª¨ë¸ í•™ìŠµ

```python
# íŒŒì´ì¬ ë¡œê¹…(logging)
logging.basicConfig(level=logging.DEBUG, 
                    format='%(asctime)s %(levelname)-8s %(message)s')

# ëª¨ë¸ í•™ìŠµ
model = SAR(
    col_user = "userID",
    col_item="itemID",
    col_rating="rating",
    col_timestamp="timestamp",
    similarity_type="jaccard", 
    time_decay_coefficient=30, 
    timedecay_formula=True,
    normalize=True
)
```

### 4 ëª¨ë¸ í•™ìŠµ ë° TEST ì…‹ ì˜ˆì¸¡ (TOP-K ì•„ì´í…œ ì¶”ì²œ)


- ì•„ì´í…œ ê°„ì˜ ë™ì‹œ ë°œìƒ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê³„ì‚°
- `Co-occurence`ëŠ” ì‚¬ìš©ìë³„ ë‘ ì•„ì´í…œì´ í•¨ê»˜ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ì˜ë¯¸
- `Co-occurence` ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ë§Œë“¤ë©´, ì•„ì´í…œ ê°„ì˜ `similarity matrix`ë¥¼ ë§Œë“ ë‹¤. (ì„ íƒí•œ ìœ ì‚¬ë„ ë°©ë²•ì— ë”°ë¼)
- ìœ ì €ì™€ ì•„ì´í…œ ê°„ì˜ ê°•ë„(strength)ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ `affinity matrix`ë¥¼ ê³„ì‚°í•œë‹¤. (ì„ í˜¸ë„ëŠ” í‰ì  ë˜ëŠ” ì˜í™”ë¥¼ ë³¸ íšŸìˆ˜, ì´ë²¤íŠ¸ì˜ ìˆ˜ ë“±)
- ì¶”ì²œì€ affinity matrix $A$ì™€ similarity matrix $S$ë¥¼ ê³±í•˜ì—¬ ìˆ˜í–‰ëœë‹¤.
- ê²°ê³¼ëŠ” ì¶”ì²œ ì ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤ $R$
- `recommend_k_items`í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ top-k ê²°ê³¼ë¥¼ ê³„ì‚°í•œë‹¤.

```python
# train í•™ìŠµ
with Timer() as train_time:
    model.fit(train)

print("Took {} seconds for training.".format(train_time.interval))
```

```python
# test ì˜ˆì¸¡
with Timer() as test_time:
    top_k = model.recommend_k_items(test,remove_seen=True)

print("Took {} seconds for prediction.".format(test_time.interval))
```

### 5 ì¶”ì²œ ì„±ëŠ¥ í‰ê°€ 
- `python_evaluation`ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ ranking ë©”íŠ¸ë¦­ì„ í†µí•´ ì„±ëŠ¥í‰ê°€
- MAP(Mean Average Precision), NDCG(Normalized Discounted Cumulative Gain), precision, and top-k ì•„ì´í…œì— ëŒ€í•œ recallì„ ì´ìš©

```python
# MAP
eval_map = map_at_k(test,top_k,col_user='userID',col_item='itemID', col_rating='rating', k=TOP_K)

# NDCG 
eval_ndcg = ndcg_at_k(test,top_k,col_user='userID',col_item='itemID', col_rating='rating', k=TOP_K)

# precision
eval_precision = precision_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)

# recall
eval_recall = recall_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)

# rmse 
eval_rmse = rmse(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')

# mae 
eval_mae = mae(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')

# rsqueared
eval_rsquared = rsquared(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')

# exp_var 
eval_exp_var = exp_var(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')

```
```python
# rating ê°’ì„ positivity_threshold ê²‚ìš¸ ì§€ì •í•´ 0ê³¼ 1ê°’ìœ¼ë¡œ ë³€í™˜
positivity_threshold = 2 # 2 ì´ˆê³¼ëŠ” 1, 2 ì´í•˜ëŠ” 0 |

test_bin = test.copy()
test_bin['rating'] = binarize(test_bin['rating'],positivity_threshold)
```

```python
# ì¶”ì²œ ì ìˆ˜(ì˜ˆì¸¡ ê°’)ì„ min-max ìŠ¤ì¼€ì¼ë§
top_k_prob = top_k.copy()

top_k_prob['prediction'] = minmax_scale(
    top_k_prob['prediction'].astype(float)

)

eval_logloss = logloss(test_bin,top_k_prob,col_user='userID',col_item='itemID',
col_rating='rating'
)
```

```python 
# ì¶”ì²œ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (offline-test)
print("Model:\t",
      "Top K:\t%d" % TOP_K,
      "MAP:\t%f" % eval_map,
      "NDCG:\t%f" % eval_ndcg,
      "Precision@K:\t%f" % eval_precision,
      "Recall@K:\t%f" % eval_recall,
      "RMSE:\t%f" % eval_rmse,
      "MAE:\t%f" % eval_mae,
      "R2:\t%f" % eval_rsquared,
      "Exp var:\t%f" % eval_exp_var,
      "Logloss:\t%f" % eval_logloss,
      sep='\n')

# íŠ¹ì • ìœ ì €ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼ 
user_id = 876

ground_truth = test[test['userID'] == user_id].sort_values(by='rating',
ascending=False)[:TOP_K]

prediction = model.recommend_k_items(pd.DataFrame(dict(userID=[user_id])),
remove_seen=True)

pd.merge(ground_truth,prediction,on=['userID', 'itemID'],how='left').sort_values(by='prediction',ascending=False)

```
### ê²°ë¡ 
- í˜‘ì—… í•„í„°ë§ì˜ ê°œë…ì„ ì´í•´í•˜ê³ , ì¶”ì²œ ì‹œìŠ¤í…œì„ ë¹ ë¥´ê²Œ ëŒë ¤ë³´ê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì¶”ì²œ ğŸ‘
- íŠ¹íˆ, ì¶”ì²œ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ëª¨ë“ˆì€ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµí•  ë•Œ ì‚¬ìš©í•˜ë©´ ì¢‹ì„ ë“¯ ğŸ‘
- í˜‘ì—… í•„í„°ë§ì´ ë­ì£ ? ê¸°ë³¸ ê°œë…ì„ ì´í•´í•˜ì§€ ëª»í•œ ì‚¬ëŒì—ê²ŒëŠ” ë¹„ì¶” ğŸ˜£ 
  (ì°¨ê·¼ì°¨ê·¼ ì‚¬ìš©ì-ì•„ì´í…œ ë§¤íŠ¸ë¦­ìŠ¤ ë§Œë“¤ê³ , ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ë“±ë“± í•œë•€í•œë•€ ì½”ë“œ ì‘ì„±í•˜ë©´ì„œ ì´í•´í•˜ëŠ” ê²ƒì„ ì¶”ì²œ)
